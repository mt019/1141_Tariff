{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa39e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 4086)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, pathlib\n",
    "\n",
    "RES_DIR = \"/home/jovyan/work/resources\"\n",
    "CONF_PATH = f\"{RES_DIR}/confusion.json\"\n",
    "MAP_PATH  = f\"{RES_DIR}/manual_map.json\"\n",
    "DOM_PATH  = f\"{RES_DIR}/domain_terms.txt\"\n",
    "\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "def _ensure_file(path, default_text):\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f: f.write(default_text)\n",
    "\n",
    "# 先放空白；之後你可累積\n",
    "_ensure_file(CONF_PATH, \"{}\\n\")\n",
    "_ensure_file(MAP_PATH, \"{}\\n\")\n",
    "_ensure_file(DOM_PATH, \"\\n\")\n",
    "\n",
    "def load_resources():\n",
    "    with open(CONF_PATH, \"r\", encoding=\"utf-8\") as f: CONFUSION = json.load(f)  # dict[str, list[str]]\n",
    "    with open(MAP_PATH, \"r\", encoding=\"utf-8\") as f: MANUAL_MAP = json.load(f)  # dict[str, str]\n",
    "    with open(DOM_PATH, \"r\", encoding=\"utf-8\") as f: DOMAIN = set(w.strip() for w in f if w.strip())\n",
    "    return CONFUSION, MANUAL_MAP, DOMAIN\n",
    "\n",
    "CONFUSION, MANUAL_MAP, DOMAIN = load_resources()\n",
    "len(CONFUSION), len(MANUAL_MAP), len(DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e36a6d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 檔數：4；MD 檔數：4\n",
      "已更新 DOMAIN 條目數：4086（+0）\n"
     ]
    }
   ],
   "source": [
    "# 需要：pip install -q pymupdf regex\n",
    "import os, re, fitz\n",
    "from collections import Counter\n",
    "\n",
    "# 路徑設定\n",
    "RES_DIR  = \"/home/jovyan/work/resources\"\n",
    "DOM_PATH = f\"{RES_DIR}/domain_terms.txt\"\n",
    "PDF_DIR  = \"/home/jovyan/work/_Material/法源\"     # ← 你的 PDF 根目錄\n",
    "MD_DIR   = \"/home/jovyan/work/mkdocs/My_Notes/01課程筆記\"          # ← 你的已修好 .md/.mdx 根目錄\n",
    "\n",
    "# 參數\n",
    "MIN_LEN = 2\n",
    "TOP_K   = 4000\n",
    "BONUS_KW = (\"關稅\",\"估價\",\"關稅估價\",\"完稅\",\"價格\",\"協定\",\"報關\",\"稅捐\",\"條\",\"第\",\"款\",\"章\",\"項\")\n",
    "\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "# 工具：列舉檔案\n",
    "def list_files(root: str, exts: tuple[str,...]):\n",
    "    out = []\n",
    "    for dp, _, fs in os.walk(root):\n",
    "        for f in fs:\n",
    "            if f.lower().endswith(exts):\n",
    "                out.append(os.path.join(dp, f))\n",
    "    return sorted(out)\n",
    "\n",
    "# 工具：MD 清洗\n",
    "fm_pat     = re.compile(r\"^---[\\s\\S]*?---\\s*\", re.MULTILINE)        # front-matter\n",
    "fence_pat  = re.compile(r\"```[\\s\\S]*?```|~~~[\\s\\S]*?~~~\", re.MULTILINE)\n",
    "indent_pat = re.compile(r\"(^ {4}.+?$)\", re.MULTILINE)\n",
    "inline_pat = re.compile(r\"`[^`]+`\")\n",
    "link_pat   = re.compile(r\"!$begin:math:display$[^$end:math:display$]*\\]$begin:math:text$[^$end:math:text$]*\\)|$begin:math:display$[^$end:math:display$]*\\]$begin:math:text$[^$end:math:text$]*\\)\")\n",
    "url_pat    = re.compile(r\"https?://\\S+\")\n",
    "html_pat   = re.compile(r\"<[^>]+>\")\n",
    "hash_pat   = re.compile(r\"^#+\\s*\", re.MULTILINE)\n",
    "tbl_pat    = re.compile(r\"^\\|.*\\|$\", re.MULTILINE)\n",
    "\n",
    "def clean_md(txt: str) -> str:\n",
    "    txt = fm_pat.sub(\" \", txt)\n",
    "    txt = fence_pat.sub(\" \", txt)\n",
    "    txt = indent_pat.sub(\" \", txt)\n",
    "    txt = inline_pat.sub(\" \", txt)\n",
    "    txt = link_pat.sub(\" \", txt)\n",
    "    txt = url_pat.sub(\" \", txt)\n",
    "    txt = html_pat.sub(\" \", txt)\n",
    "    txt = hash_pat.sub(\"\", txt)\n",
    "    txt = tbl_pat.sub(\" \", txt)\n",
    "    return txt\n",
    "\n",
    "token_pat = re.compile(rf'[\\u4e00-\\u9fffA-Za-z0-9]{{{MIN_LEN},}}')\n",
    "\n",
    "# 從 PDF 抽詞\n",
    "def extract_terms_from_pdfs(paths, top_k=4000):\n",
    "    freq = Counter()\n",
    "    for p in paths:\n",
    "        try:\n",
    "            doc = fitz.open(p)\n",
    "            for pg in doc:\n",
    "                t = pg.get_text(\"text\") or \"\"\n",
    "                for w in token_pat.findall(t):\n",
    "                    freq[w] += 3 if any(b in w for b in BONUS_KW) else 1\n",
    "        except Exception as e:\n",
    "            print(\"略過無法讀取：\", p, \"|\", e)\n",
    "    return [w for w,_ in freq.most_common(top_k)]\n",
    "\n",
    "# 從 MD 抽詞\n",
    "def extract_terms_from_mds(paths, top_k=4000):\n",
    "    freq = Counter()\n",
    "    for p in paths:\n",
    "        try:\n",
    "            t = open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "            t = clean_md(t)\n",
    "            for w in token_pat.findall(t):\n",
    "                freq[w] += 3 if any(b in w for b in BONUS_KW) else 1\n",
    "        except Exception as e:\n",
    "            print(\"略過無法讀取：\", p, \"|\", e)\n",
    "    return [w for w,_ in freq.most_common(top_k)]\n",
    "\n",
    "# 執行\n",
    "pdfs = list_files(PDF_DIR, (\".pdf\",))\n",
    "mds  = list_files(MD_DIR, (\".md\", \".mdx\"))\n",
    "\n",
    "print(f\"PDF 檔數：{len(pdfs)}；MD 檔數：{len(mds)}\")\n",
    "\n",
    "terms_pdf = extract_terms_from_pdfs(pdfs, top_k=TOP_K)\n",
    "terms_md  = extract_terms_from_mds(mds,  top_k=TOP_K)\n",
    "\n",
    "# 合併既有 DOMAIN\n",
    "domain = set()\n",
    "if os.path.exists(DOM_PATH):\n",
    "    with open(DOM_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        domain |= {x.strip() for x in f if x.strip()}\n",
    "\n",
    "before = len(domain)\n",
    "domain |= set(terms_pdf)\n",
    "domain |= set(terms_md)\n",
    "\n",
    "with open(DOM_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sorted(domain)))\n",
    "\n",
    "print(f\"已更新 DOMAIN 條目數：{len(domain)}（+{len(domain)-before}）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5edb2d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# pip install -q transformers torch rapidfuzz regex pangu zhon\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch, re, difflib, pangu\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\").eval()\n",
    "\n",
    "ZH_PUNC = {\",\":\"，\",\".\":\"。\",\"?\":\"？\",\"!\":\"！\",\":\":\"：\",\";\":\"；\"}\n",
    "def norm_punc(s):\n",
    "    s = s.translate(str.maketrans(ZH_PUNC))\n",
    "    for p in \"，。？！：；\": s = s.replace(\" \"+p, p)\n",
    "    return pangu.spacing_text(s)\n",
    "\n",
    "def mlm_topk_char(sent_chars, i, k=5):\n",
    "    # Restrict context to model max length with a centered window.\n",
    "    max_len = getattr(mlm.config, 'max_position_embeddings', 512)\n",
    "    window = max(2, max_len - 2)  # reserve [CLS],[SEP]\n",
    "    left = (window - 1) // 2\n",
    "    right = window - 1 - left\n",
    "    start = max(0, i - left)\n",
    "    end = min(len(sent_chars), i + 1 + right)\n",
    "    sub_chars = sent_chars[start:end]\n",
    "    # Tokenize with truncation and get masks/type ids to avoid buffer mismatch.\n",
    "    t = tok(sub_chars, is_split_into_words=True, return_tensors=\"pt\", add_special_tokens=True, truncation=True, max_length=max_len)\n",
    "    ids = t[\"input_ids\"][0].clone()\n",
    "    pos = (i - start) + 1  # [CLS] offset within the window\n",
    "    ids[pos] = tok.mask_token_id\n",
    "    # Prepare inputs with attention and token_type ids.\n",
    "    inputs = {\"input_ids\": ids.unsqueeze(0)}\n",
    "    if \"token_type_ids\" in t:\n",
    "        inputs[\"token_type_ids\"] = t[\"token_type_ids\"]\n",
    "    else:\n",
    "        inputs[\"token_type_ids\"] = torch.zeros_like(ids).unsqueeze(0)\n",
    "    if \"attention_mask\" in t:\n",
    "        inputs[\"attention_mask\"] = t[\"attention_mask\"]\n",
    "    else:\n",
    "        inputs[\"attention_mask\"] = torch.ones_like(ids).unsqueeze(0)\n",
    "    with torch.inference_mode():\n",
    "        logits = mlm(**inputs).logits[0, pos]\n",
    "    topk = torch.topk(logits, k)\n",
    "    return [tok.convert_ids_to_tokens(int(x)) for x in topk.indices]\n",
    "\n",
    "def protect_terms(text, terms):\n",
    "    for t in sorted(terms, key=len, reverse=True):\n",
    "        text = text.replace(t, f\"⟦{t}⟧\")\n",
    "    return text\n",
    "\n",
    "def unprotect_terms(text):\n",
    "    return text.replace(\"⟦\",\"\").replace(\"⟧\",\"\")\n",
    "\n",
    "def correct_sentence(sent, CONFUSION, MANUAL_MAP, DOMAIN):\n",
    "    # 詞級先行（你的人工作業會逐步累積到 MANUAL_MAP）\n",
    "    for a,b in sorted(MANUAL_MAP.items(), key=lambda x: len(x[0]), reverse=True):\n",
    "        sent = re.sub(re.escape(a), b, sent)\n",
    "    s = protect_terms(sent, DOMAIN)\n",
    "    chars = list(s)\n",
    "    for i,ch in enumerate(chars):\n",
    "        if ch in \"⟦⟧\": continue\n",
    "        # 候選取「混淆表」如有，否則允許原字\n",
    "        cand_pool = CONFUSION.get(ch, [])\n",
    "        if ch not in cand_pool: cand_pool = [ch] + list(cand_pool)\n",
    "        # 用 MLM 打分取得高機率候選，與候選池交集\n",
    "        topk = mlm_topk_char(chars, i, k=6)\n",
    "        cands = [c for c in topk if c in cand_pool]\n",
    "        if cands and cands[0] != ch:\n",
    "            chars[i] = cands[0]\n",
    "    out = unprotect_terms(\"\".join(chars))\n",
    "    # 對不在 DOMAIN 的相近詞拉回（可選）\n",
    "    if DOMAIN:\n",
    "        words = set(re.findall(r'[\\u4e00-\\u9fffA-Za-z0-9]{2,}', out))\n",
    "        for w in words:\n",
    "            if w not in DOMAIN:\n",
    "                hit = process.extractOne(w, DOMAIN, scorer=fuzz.WRatio)\n",
    "                if hit and hit[1] >= 93:\n",
    "                    out = re.sub(re.escape(w), hit[0], out)\n",
    "    return norm_punc(out)\n",
    "\n",
    "def split_sents(text):\n",
    "    return [p for p in re.split(r'(?<=[。！？\\?])\\s+|\\n+', text) if p.strip()]\n",
    "\n",
    "def correct_text(text, CONFUSION, MANUAL_MAP, DOMAIN):\n",
    "    return \"\\n\".join(correct_sentence(s, CONFUSION, MANUAL_MAP, DOMAIN) for s in split_sents(text))\n",
    "\n",
    "def show_diff(a, b):\n",
    "    return \"\".join(difflib.unified_diff(a.splitlines(1), b.splitlines(1), fromfile=\"orig.txt\", tofile=\"fixed.txt\", lineterm=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH  = \"/home/jovyan/work/mkdocs/My_Notes/逐字稿/W03-0915-1逐字稿初稿.md\"   # 改成你的檔案\n",
    "OUT_PATH = \"/home/jovyan/work/mkdocs/My_Notes/逐字稿/W03-0915-1逐字稿-auto_fixed.txt\"\n",
    "\n",
    "raw = open(IN_PATH, \"r\", encoding=\"utf-8\").read()\n",
    "CONFUSION, MANUAL_MAP, DOMAIN = load_resources()\n",
    "fixed = correct_text(raw, CONFUSION, MANUAL_MAP, DOMAIN)\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "open(OUT_PATH, \"w\", encoding=\"utf-8\").write(fixed)\n",
    "print(show_diff(raw, fixed))\n",
    "print(\"輸出：\", OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_manual_map(pairs):\n",
    "    \"\"\"\n",
    "    pairs: List[Tuple['錯','正']]\n",
    "    \"\"\"\n",
    "    mp = json.load(open(MAP_PATH,\"r\",encoding=\"utf-8\"))\n",
    "    for a,b in pairs:\n",
    "        if a and b and a!=b:\n",
    "            mp[a] = b\n",
    "    with open(MAP_PATH,\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump(mp, f, ensure_ascii=False, indent=2)\n",
    "    print(\"已更新 manual_map.json，共\", len(mp), \"條\")\n",
    "\n",
    "# 範例：把你這輪人工更正回寫\n",
    "# update_manual_map([(\"上至\", \"上字\"), (\"完稅價\", \"完稅價格\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
