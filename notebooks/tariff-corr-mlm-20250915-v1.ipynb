{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa39e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, pathlib\n",
    "\n",
    "RES_DIR = \"/home/jovyan/work/resources\"\n",
    "CONF_PATH = f\"{RES_DIR}/confusion.json\"\n",
    "MAP_PATH  = f\"{RES_DIR}/manual_map.json\"\n",
    "DOM_PATH  = f\"{RES_DIR}/domain_terms.txt\"\n",
    "\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "def _ensure_file(path, default_text):\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f: f.write(default_text)\n",
    "\n",
    "# 先放空白；之後你可累積\n",
    "_ensure_file(CONF_PATH, \"{}\\n\")\n",
    "_ensure_file(MAP_PATH, \"{}\\n\")\n",
    "_ensure_file(DOM_PATH, \"\\n\")\n",
    "\n",
    "def load_resources():\n",
    "    with open(CONF_PATH, \"r\", encoding=\"utf-8\") as f: CONFUSION = json.load(f)  # dict[str, list[str]]\n",
    "    with open(MAP_PATH, \"r\", encoding=\"utf-8\") as f: MANUAL_MAP = json.load(f)  # dict[str, str]\n",
    "    with open(DOM_PATH, \"r\", encoding=\"utf-8\") as f: DOMAIN = set(w.strip() for w in f if w.strip())\n",
    "    return CONFUSION, MANUAL_MAP, DOMAIN\n",
    "\n",
    "CONFUSION, MANUAL_MAP, DOMAIN = load_resources()\n",
    "len(CONFUSION), len(MANUAL_MAP), len(DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要：pip install -q pymupdf regex\n",
    "import os, re, fitz\n",
    "from collections import Counter\n",
    "\n",
    "RES_DIR  = \"/home/jovyan/work/resources\"\n",
    "DOM_PATH = f\"{RES_DIR}/domain_terms.txt\"\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "PDF_DIR = \"/home/jovyan/work/pdfs\"   # ← 改成你的PDF根目錄\n",
    "MIN_LEN = 2                          # 最小詞長\n",
    "TOP_K   = 4000                       # 保留前TOP_K高頻詞\n",
    "\n",
    "def list_pdfs(root: str):\n",
    "    out = []\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".pdf\"):\n",
    "                out.append(os.path.join(dirpath, f))\n",
    "    return sorted(out)\n",
    "\n",
    "def extract_terms_from_pdfs(paths, min_len=2, top_k=4000):\n",
    "    pat = re.compile(rf'[\\u4e00-\\u9fffA-Za-z0-9]{{{min_len},}}')\n",
    "    freq = Counter()\n",
    "    for p in paths:\n",
    "        try:\n",
    "            doc = fitz.open(p)\n",
    "            for pg in doc:\n",
    "                t = pg.get_text(\"text\") or \"\"\n",
    "                for w in pat.findall(t):\n",
    "                    freq[w] += 1\n",
    "        except Exception as e:\n",
    "            print(\"略過無法讀取：\", p, \"|\", e)\n",
    "    # 對關稅/法規常見詞加權，讓專有詞更容易進清單\n",
    "    bonus_kw = (\"關稅\",\"估價\",\"關稅估價\",\"完稅\",\"價格\",\"協定\",\"報關\",\"稅捐\",\"條\",\"第\",\"款\",\"章\",\"項\")\n",
    "    for k in list(freq):\n",
    "        if any(b in k for b in bonus_kw):\n",
    "            freq[k] *= 3\n",
    "    return [w for w,_ in freq.most_common(top_k)]\n",
    "\n",
    "pdfs = list_pdfs(PDF_DIR)\n",
    "print(\"PDF 檔數：\", len(pdfs))\n",
    "\n",
    "terms = extract_terms_from_pdfs(pdfs, min_len=MIN_LEN, top_k=TOP_K)\n",
    "\n",
    "# 合併到既有 DOMAIN（若檔案存在）\n",
    "domain = set()\n",
    "if os.path.exists(DOM_PATH):\n",
    "    with open(DOM_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        domain |= {x.strip() for x in f if x.strip()}\n",
    "\n",
    "domain |= set(terms)\n",
    "\n",
    "with open(DOM_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sorted(domain)))\n",
    "\n",
    "print(\"已更新 DOMAIN 條目數：\", len(domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28218cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q pymupdf regex\n",
    "import fitz, re\n",
    "\n",
    "PDFS = [\n",
    "    # 範例：把你的關稅/法規 PDF 路徑放進來\n",
    "    # \"/home/jovyan/work/pdfs/關稅法.pdf\",\n",
    "]\n",
    "\n",
    "def extract_terms_from_pdfs(paths, min_len=2, top_k=2000):\n",
    "    freq = {}\n",
    "    pat = re.compile(r'[\\u4e00-\\u9fffA-Za-z0-9]{%d,}' % min_len)\n",
    "    for p in paths:\n",
    "        try:\n",
    "            doc = fitz.open(p)\n",
    "            for pg in doc:\n",
    "                t = pg.get_text(\"text\") or \"\"\n",
    "                for w in pat.findall(t):\n",
    "                    freq[w] = freq.get(w,0) + 1\n",
    "        except Exception as e:\n",
    "            print(\"skip:\", p, e)\n",
    "    # 簡易過濾：數字+「條」「第」「款」「關稅」「估價」等優先\n",
    "    bonus = (\"關稅\",\"估價\",\"條\",\"第\",\"款\",\"稅捐\",\"貨物\",\"完稅\",\"價格\",\"協定\",\"報關\")\n",
    "    for k in list(freq):\n",
    "        if any(b in k for b in bonus):\n",
    "            freq[k] *= 3\n",
    "    terms = [w for w,_ in sorted(freq.items(), key=lambda x: -x[1])[:top_k]]\n",
    "    return terms\n",
    "\n",
    "if PDFS:\n",
    "    new_terms = extract_terms_from_pdfs(PDFS, top_k=4000)\n",
    "    DOMAIN.update(new_terms)\n",
    "    with open(DOM_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(sorted(DOMAIN)))\n",
    "    print(\"DOMAIN 條目數：\", len(DOMAIN))\n",
    "else:\n",
    "    print(\"未提供 PDF，跳過抽詞。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers torch rapidfuzz regex pangu zhon\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch, re, difflib, pangu\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\").eval()\n",
    "\n",
    "ZH_PUNC = {\",\":\"，\",\".\":\"。\",\"?\":\"？\",\"!\":\"！\",\":\":\"：\",\";\":\"；\"}\n",
    "def norm_punc(s):\n",
    "    s = s.translate(str.maketrans(ZH_PUNC))\n",
    "    for p in \"，。？！：；\": s = s.replace(\" \"+p, p)\n",
    "    return pangu.spacing_text(s)\n",
    "\n",
    "def mlm_topk_char(sent_chars, i, k=5):\n",
    "    t = tok(sent_chars, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    ids = t[\"input_ids\"][0].clone()\n",
    "    pos = i + 1  # [CLS] 偏移\n",
    "    ids[pos] = tok.mask_token_id\n",
    "    with torch.inference_mode():\n",
    "        logits = mlm(input_ids=ids.unsqueeze(0)).logits[0, pos]\n",
    "    topk = torch.topk(logits, k)\n",
    "    return [tok.convert_ids_to_tokens(int(x)) for x in topk.indices]\n",
    "\n",
    "def protect_terms(text, terms):\n",
    "    for t in sorted(terms, key=len, reverse=True):\n",
    "        text = text.replace(t, f\"⟦{t}⟧\")\n",
    "    return text\n",
    "\n",
    "def unprotect_terms(text):\n",
    "    return text.replace(\"⟦\",\"\").replace(\"⟧\",\"\")\n",
    "\n",
    "def correct_sentence(sent, CONFUSION, MANUAL_MAP, DOMAIN):\n",
    "    # 詞級先行（你的人工作業會逐步累積到 MANUAL_MAP）\n",
    "    for a,b in sorted(MANUAL_MAP.items(), key=lambda x: len(x[0]), reverse=True):\n",
    "        sent = re.sub(re.escape(a), b, sent)\n",
    "    s = protect_terms(sent, DOMAIN)\n",
    "    chars = list(s)\n",
    "    for i,ch in enumerate(chars):\n",
    "        if ch in \"⟦⟧\": continue\n",
    "        # 候選取「混淆表」如有，否則允許原字\n",
    "        cand_pool = CONFUSION.get(ch, [])\n",
    "        if ch not in cand_pool: cand_pool = [ch] + list(cand_pool)\n",
    "        # 用 MLM 打分取得高機率候選，與候選池交集\n",
    "        topk = mlm_topk_char(chars, i, k=6)\n",
    "        cands = [c for c in topk if c in cand_pool]\n",
    "        if cands and cands[0] != ch:\n",
    "            chars[i] = cands[0]\n",
    "    out = unprotect_terms(\"\".join(chars))\n",
    "    # 對不在 DOMAIN 的相近詞拉回（可選）\n",
    "    if DOMAIN:\n",
    "        words = set(re.findall(r'[\\u4e00-\\u9fffA-Za-z0-9]{2,}', out))\n",
    "        for w in words:\n",
    "            if w not in DOMAIN:\n",
    "                hit = process.extractOne(w, DOMAIN, scorer=fuzz.WRatio)\n",
    "                if hit and hit[1] >= 93:\n",
    "                    out = re.sub(re.escape(w), hit[0], out)\n",
    "    return norm_punc(out)\n",
    "\n",
    "def split_sents(text):\n",
    "    return [p for p in re.split(r'(?<=[。！？\\?])\\s+|\\n+', text) if p.strip()]\n",
    "\n",
    "def correct_text(text, CONFUSION, MANUAL_MAP, DOMAIN):\n",
    "    return \"\\n\".join(correct_sentence(s, CONFUSION, MANUAL_MAP, DOMAIN) for s in split_sents(text))\n",
    "\n",
    "def show_diff(a, b):\n",
    "    return \"\".join(difflib.unified_diff(a.splitlines(1), b.splitlines(1), fromfile=\"orig.txt\", tofile=\"fixed.txt\", lineterm=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH  = \"/home/jovyan/work/transcripts/raw_trad.txt\"   # 改成你的檔案\n",
    "OUT_PATH = \"/home/jovyan/work/transcripts/auto_fixed.txt\"\n",
    "\n",
    "raw = open(IN_PATH, \"r\", encoding=\"utf-8\").read()\n",
    "CONFUSION, MANUAL_MAP, DOMAIN = load_resources()\n",
    "fixed = correct_text(raw, CONFUSION, MANUAL_MAP, DOMAIN)\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "open(OUT_PATH, \"w\", encoding=\"utf-8\").write(fixed)\n",
    "print(show_diff(raw, fixed))\n",
    "print(\"輸出：\", OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_manual_map(pairs):\n",
    "    \"\"\"\n",
    "    pairs: List[Tuple['錯','正']]\n",
    "    \"\"\"\n",
    "    mp = json.load(open(MAP_PATH,\"r\",encoding=\"utf-8\"))\n",
    "    for a,b in pairs:\n",
    "        if a and b and a!=b:\n",
    "            mp[a] = b\n",
    "    with open(MAP_PATH,\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump(mp, f, ensure_ascii=False, indent=2)\n",
    "    print(\"已更新 manual_map.json，共\", len(mp), \"條\")\n",
    "\n",
    "# 範例：把你這輪人工更正回寫\n",
    "# update_manual_map([(\"上至\", \"上字\"), (\"完稅價\", \"完稅價格\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
