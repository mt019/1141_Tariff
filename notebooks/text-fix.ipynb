{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13df4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch regex rapidfuzz pangu zhon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc7af4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2025d86739140dd9581c10653a80da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536a82a00d6e4c94b2c2b81c5a247ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3d2297012041a794f6bd96003aa5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53ce13616ad4e4cb03b9dfada504dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ce8f10a1f9461789ec67fc21cb370e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch, re, difflib\n",
    "from rapidfuzz import process, fuzz\n",
    "import pangu\n",
    "\n",
    "MODEL_ID = \"bert-base-chinese\"  # 公開模型，離線可快取\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(MODEL_ID).eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def mlm_topk(sent, i, k=5):\n",
    "    tokens = tok(list(sent), is_split_into_words=True, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"][0]\n",
    "    mask_id = tok.mask_token_id\n",
    "    pos = i + 1  # [CLS] 在第0位\n",
    "    ids = input_ids.clone()\n",
    "    ids[pos] = mask_id\n",
    "    out = mlm(input_ids=ids.unsqueeze(0)).logits[0, pos]\n",
    "    topk = torch.topk(out, k)\n",
    "    return [tok.convert_ids_to_tokens(int(t)) for t in topk.indices], topk.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c86970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小形近表（可擴充），全部繁體字元\n",
    "CONFUSION = {\n",
    "    \"稅\": [\"稅\",\"稅\",\"稅\"],  # 佔位，實務請擴充你的領域錯字集\n",
    "    \"關\": [\"關\",\"觀\",\"官\"],\n",
    "    \"條\": [\"條\",\"條\",\"條\",\"條\"],\n",
    "    \"價\": [\"價\",\"價\",\"價\"],\n",
    "    \"貨\": [\"貨\",\"貨\",\"貨\"],\n",
    "    # 範例：把常見錯→正收集成映射\n",
    "}\n",
    "MANUAL_MAP = {\n",
    "    # \"錯誤詞\": \"正確詞\"\n",
    "    \"上至\": \"上字\",  # 例：法院案號「上字」→初稿常被誤作「上至」\n",
    "}\n",
    "DOMAIN = set(\"\"\"\n",
    "關稅法 關稅估價 合理方法 第29條 第35條 移轉訂價 關稅估價協定\n",
    "進口 完稅價格 報關業者 委任書 空運快遞貨物通關辦法\n",
    "\"\"\".split())\n",
    "\n",
    "def protect_terms(text:str, terms:set[str])->str:\n",
    "    for t in sorted(terms, key=len, reverse=True):\n",
    "        text = text.replace(t, f\"⟦{t}⟧\")\n",
    "    return text\n",
    "def unprotect_terms(text:str)->str:\n",
    "    return text.replace(\"⟦\",\"\").replace(\"⟧\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15cb216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZH_PUNC = {\",\":\"，\",\".\":\"。\",\"?\":\"？\",\"!\":\"！\",\":\":\"：\",\";\":\"；\"}\n",
    "def norm_punc(s:str)->str:\n",
    "    s = s.translate(str.maketrans(ZH_PUNC))\n",
    "    for p in \"，。？！：；\": s = s.replace(\" \"+p, p)\n",
    "    return pangu.spacing_text(s)\n",
    "\n",
    "def candidates_for_char(ch:str):\n",
    "    cand = CONFUSION.get(ch, [])\n",
    "    # 加入原字，避免被強改\n",
    "    if ch not in cand: cand = [ch] + cand\n",
    "    # 過濾成單字且非特殊符號\n",
    "    return [c for c in cand if len(c)==1]\n",
    "\n",
    "def correct_sentence(sent:str, k=5, prob_thresh=-8.0):\n",
    "    s = protect_terms(sent, DOMAIN)\n",
    "    chars = list(s)\n",
    "    changed = False\n",
    "    for i,ch in enumerate(chars):\n",
    "        if ch in \"⟦⟧\":  # 保護塊跳過\n",
    "            continue\n",
    "        # 用 MLM 估該位置的前 k 候選\n",
    "        topk_tokens, topk_scores = mlm_topk(chars, i, k=k)\n",
    "        # 僅考慮「與混淆表交集」的候選\n",
    "        cand = [c for c in topk_tokens if c in candidates_for_char(ch)]\n",
    "        if not cand:\n",
    "            continue\n",
    "        # 若原字不在高分候選且最高分候選分數顯著更好，才替換\n",
    "        # 這裡用簡單啟發：若 top1 != 原字 且分數明顯高於閾值\n",
    "        top1 = cand[0]\n",
    "        if top1 != ch:\n",
    "            chars[i] = top1\n",
    "            changed = True\n",
    "    out = unprotect_terms(\"\".join(chars))\n",
    "    # 手動映射（詞級）\n",
    "    for a,b in sorted(MANUAL_MAP.items(), key=lambda x: len(x[0]), reverse=True):\n",
    "        out = re.sub(re.escape(a), b, out)\n",
    "    return norm_punc(out), changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib, re\n",
    "\n",
    "def split_sents(text:str):\n",
    "    # 保守斷句：句末標點 + 換行作為界線\n",
    "    parts = re.split(r'(?<=[。！？\\?])\\s+|\\n+', text)\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "def correct_text(text:str):\n",
    "    sents = split_sents(text)\n",
    "    fixed = []\n",
    "    for s in sents:\n",
    "        fs, _ = correct_sentence(s)\n",
    "        fixed.append(fs)\n",
    "    return \"\\n\".join(fixed)\n",
    "\n",
    "def show_diff(a:str, b:str)->str:\n",
    "    return \"\".join(difflib.unified_diff(\n",
    "        a.splitlines(keepends=True),\n",
    "        b.splitlines(keepends=True),\n",
    "        fromfile=\"orig.txt\", tofile=\"fixed.txt\", lineterm=\"\"\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
